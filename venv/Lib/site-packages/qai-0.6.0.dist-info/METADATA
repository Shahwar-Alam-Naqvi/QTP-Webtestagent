Metadata-Version: 2.1
Name: qai
Version: 0.6.0
Summary: 
Author: vatsalsqyrus
Author-email: vatsals@quinnox.com
Requires-Python: >=3.10,<4.0
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Dist: aioboto3 (>=13.1.1,<14.0.0)
Requires-Dist: aiobotocore[boto3] (>=2.13.1,<3.0.0)
Requires-Dist: anthropic[bedrock] (>=0.31.0,<0.32.0)
Requires-Dist: backoff (>=2.2.1,<3.0.0)
Requires-Dist: claudetools (==1.0.2)
Requires-Dist: httpx (>=0.27.0,<0.28.0)
Requires-Dist: jinja2 (>=3.1.4,<4.0.0)
Requires-Dist: json-repair (>=0.25.3,<0.26.0)
Requires-Dist: openai (>=1.35.13,<2.0.0)
Requires-Dist: pillow (>=10.4.0,<11.0.0)
Requires-Dist: pydantic (>=2.8.2,<3.0.0)
Requires-Dist: python-dotenv (>=1.0.1,<2.0.0)
Requires-Dist: tiktoken (>=0.7.0,<0.8.0)
Requires-Dist: tokenizers (>=0.19.1,<0.20.0)
Requires-Dist: transformers (>=4.43.1,<5.0.0)
Description-Content-Type: text/markdown

# QAI

An internal - **Qyrus only** - wrapper for accessing _AzureOpenAI, Claude, and Bedrock (Llama3)_ models.

## How to Install?

Use the following command to install the `qai` package.

```
pip install git+https://github.com/QQyrus/qai.git@staging # remove @staging for stable release
```

> **Note**: Only `async` version are available with QAI as we don't want to use Python concurrency as best as we can.

> **SSL Error:** In certail systems one can encounter the following _SSL Certificate Error_.
> ![SSL Certificate Error Example](./docs/error-qai-ssl-download.png)
> **Resolution:** To resolve this run the following command before the `pip install` command.
>```sh
>git config --global http.sslVerify false
>```

## How To Use QAI

### Setup Environment Variables

The following environment variables are required use either of Azure OpenAI or Claude (via Bedrock).

```
OPENAI_API_TYPE="azure"
AZURE_OPENAI_ENDPOINTS="YOUR_ENDPOINT_1,YOUR_ENDPOINT_2,YOUR_ENDPOINT_n"
AZURE_OPENAI_KEYS="API_KEY_FOR_ENDPOINT_1,API_KEY_FOR_ENDPOINT_2,API_KEY_FOR_ENDPOINT_n"
AZURE_DEPLOYMENT_MAPS="gpt-4-vision-preview:gpt4128kV,gpt-3.5-turbo-1106:gpt3516k,gpt-4-1106-preview:gpt4128k,gpt-4o:gpt4ogs" # model names to deployment name mapping `openai_model_name:deployment_name`
OPENAI_API_VERSION="2024-02-01"

AWS_BEDROCK_ACCESS_KEY="YOUR-AWS-ACCESS-KEY"
AWS_BEDROCK_SECRET_KEY="YOUR-AWS-SECRET-KEY"
AWS_BEDROCK_REGION="YOUR-AWS-REGION"
```

### Setup
First, make sure to import the necessary modules and initialize the `QAILLMs` instance:

```python
import os
import asyncio
import nest_asyncio
from qai import QAILLMs

nest_asyncio.apply()

llm = QAILLMs()
```

### Normal Completion

#### Azure OpenAI

To perform a normal completion with Azure OpenAI:

```python
asyncio.run(
    llm.openai.llm.__complete__("gpt-4o",
                                [{
                                    "role": "system",
                                    "content": "You're a helpful assistant"
                                }, {
                                    "role": "user",
                                    "content": "Hi"
                                }]))  
```

#### Bedrock Claude

To perform a normal completion with Bedrock Claude:

```python
asyncio.run(
    llm.claude.llm.__complete__("anthropic.claude-3-sonnet-20240229-v1:0",
                                [{
                                    "role": "system",
                                    "content": "You're a helpful assistant"
                                }, {
                                    "role": "user",
                                    "content": "Hi"
                                }],
                                max_tokens=128))
```

#### Bedrock Llama 3.1

To perform a normal completion with Bedrock Llama 3.1:

```python
asyncio.run(
    llm.llama.llm.__complete__("meta.llama3-1-8b-instruct-v1:0",
                                [{
                                    "role": "system",
                                    "content": "You're a helpful assistant"
                                }, {
                                    "role": "user",
                                    "content": "Hi"
                                }],
                                max_gen_len=128))
```

#### Provider as an argument

- The provider can be set between "openai" or "claude".

```python
provider = "openai"
asyncio.run(
    getattr(llm, provider).llm.__complete__("gpt-4o",
                                [{
                                    "role": "system",
                                    "content": "You're a helpful assistant"
                                }, {
                                    "role": "user",
                                    "content": "Hi"
                                }])
)

```

### Function Calling

Define the `GetWeather` function model using Pydantic:

```python
from pydantic import BaseModel, Field

class GetWeather(BaseModel):
    city: str = Field("...", description="City name to get the weather.")
```

Define the function call schema:

```python
fcs = [{
    "name": "GetWeather",
    "description": "Get Weather for a city",
    "parameters": GetWeather.model_json_schema()
}]
```

#### Bedrock Claude

Perform a function call with Bedrock Claude:

```python
messages = [{
    "role": "system",
    "content": "The user will ask for the weather of a city, you have use the appropriate function to fulfil the request."
}, {
    "role": "user",
    "content": "How's the weather in Bangalore, India"
}]

print(
    asyncio.run(
        llm.claude.llm.__function_call__(
            "anthropic.claude-3-sonnet-20240229-v1:0",
            messages,
            fcs,
            tool_choice=None,
            max_tokens=2046)))
```

#### Bedrock Llama 3.1

Perform a function call with Bedrock Llama 3.1:

```python
print(
    asyncio.run(
        llm.llama.llm.__function_call__("meta.llama3-8b-instruct-v1:0",
                                         messages,
                                         fcs,
                                         tool_choice=None,
                                         max_gen_len=2046)))
```

#### Provider as an argument

- The provider can be set between "openai" or "claude".

```python
provider = "claude"
print(
    asyncio.run(
        getattr(llm, provider).llm.__function_call__("anthropic.claude-3-sonnet-20240229-v1:0",
                                         messages,
                                         fcs,
                                         tool_choice=None,
                                         max_tokens=2046))
)
```

#### Azure OpenAI

Perform a function call with Azure OpenAI:

```python
print(
    asyncio.run(
        llm.openai.llm.__function_call__("gpt-4o",
                                         messages,
                                         fcs,
                                         tool_choice=None,
                                         max_tokens=2046)))
```

### Streaming

Define an asynchronous streaming function:

```python
from typing import Union, Literal

async def stream(
    message: str,
    model: str,
    provider: Literal["claude", "openai"],
    max_tokens: int = 128,
):
    async for chunk in getattr(llm, provider).llm.__stream__(
            model,
            [{
                "role": "system",
                "content": "You're a helpful assistant"
            }, {
                "role": "user",
                "content": message
            }],
            max_tokens=max_tokens):
        print(chunk, end="", flush=True)
```

#### Bedrock Claude

Perform streaming with Bedrock Claude:

```python
asyncio.run(stream("help we write code in Python", "anthropic.claude-3-sonnet-20240229-v1:0", "claude", 512))
```

#### Azure OpenAI

Perform streaming with Azure OpenAI:

```python
asyncio.run(stream("help we write code in Python, I'm looking to write an API in FastAPI", "gpt-4o", "openai", 512))
```

### Bedrock Llama 3.1

```python
asyncio.run(
    stream(
        "help we write code in Python, I'm looking to write an API in FastAPI",
        "meta.llama3-8b-instruct-v1:0",
        "llama",
        max_gen_len=512))
```

> **Note:** No need to use `asyncio.run` when using with async routes in FastAPI or Flask.
