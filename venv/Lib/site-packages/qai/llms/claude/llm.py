import asyncio
import json
import traceback
import backoff
import logging
from qai.llms.base import BaseLLM
from qai.llms.claude.tokenizer.context import ClaudeContextManagement
from anthropic import AsyncAnthropicBedrock, RateLimitError, APIConnectionError
from typing import List, Dict, Union, Tuple
# from configs import AWS_ACCESS_KEY, AWS_ACCESS_SECRET_KEY, AWS_BEDROCK_REGION
from qai.configs import (AWS_BEDROCK_SECRET_KEY, AWS_BEDROCK_ACCESS_KEY,
                         AWS_BEDROCK_REGION, MODEL_RETRIES)
from claudetools.tools.tool import AsyncTool


class ClaudeLLM(BaseLLM):
    """
    ClaudeLLM is a specialized implementation of the BaseLLM which interacts with Claude.

    Args:
        kwargs: Arbitrary keyword arguments that may include:
            - aws_access_key (str): AWS access key ID. If not provided, it defaults to the value of the AWS_BEDROCK_ACCESS_KEY environment variable.
            - aws_secret_key (str): AWS secret access key. If not provided, it defaults to the value of the AWS_BEDROCK_SECRET_KEY environment variable.
            - aws_bedrock_region (str): AWS region name. If not provided, it defaults to the value of the AWS_BEDROCK_REGION environment variable.

    Attributes:
        extra_args (dict): Any extra arguments passed during initialization.
        tool (AsyncTool): AsyncTool instance initialized with the provided AWS credentials.
        ctx_mgmt (ClaudeContextManagement): Instance of ClaudeContextManagement for handling context management.
    """

    def __init__(self, **kwargs):
        """
        Initializes an instance of ClaudeLLM with the provided arguments or environment variables.

        Args:
            kwargs: Arbitrary keyword arguments that may include:
                - aws_access_key (str): AWS access key ID.
                - aws_secret_key (str): AWS secret access key.
                - aws_bedrock_region (str): AWS region name.

        The following environment variables will be used if the corresponding arguments are not provided:
            - AWS_BEDROCK_ACCESS_KEY
            - AWS_BEDROCK_SECRET_KEY
            - AWS_BEDROCK_REGION
        """
        self.extra_args = kwargs
        self.tool = AsyncTool(
            aws_access_key=kwargs.get("aws_access_key",
                                      AWS_BEDROCK_ACCESS_KEY),
            aws_secret_key=kwargs.get("aws_secret_key",
                                      AWS_BEDROCK_SECRET_KEY),
            aws_region=kwargs.get("aws_bedrock_region", AWS_BEDROCK_REGION))
        self.ctx_mgmt = ClaudeContextManagement()

    async def __stream__(self, model: str, messages: List[Dict], **kwargs):
        """
        Asynchronously streams responses from the Claude model.

        Args:
            model (str): The model identifier to use.
            messages (List[Dict]): List of message dictionaries to send to the model.
            kwargs: Arbitrary keyword arguments that may include:
                - allowed_context_length (int): Maximum allowed length for the context. Defaults to 150,000 tokens.
                - max_tokens (int): Maximum number of tokens to generate.

        Yields:
            str: Partial content of the response as it is being streamed.
        """
        system_message = list(
            filter(lambda message: message.get("role") == "system", messages))
        system_message = system_message[0].get("content")
        print(
            f"SYSTEM MESSAGE TOKENS: {self.ctx_mgmt.__count_tokens__(system_message)}"
        )
        messages = messages[1:]
        allowed_max_length = kwargs.get("allowed_context_length", 150_000)
        if "allowed_context_length" in kwargs:
            del kwargs["allowed_context_length"]
        else:
            logging.info(
                f"MAXIMUM ALLOWED CONTEXT LENGTH NOT SPECIFIED FOR OPENAI. Using 150,000"
            )
        ctx_managed_messages = self.ctx_mgmt(
            messages, allowed_max_length,
            self.ctx_mgmt.__count_tokens__(system_message))
        stream = await self.tool.complete.client.messages.create(
            max_tokens=kwargs.get("max_tokens"),
            messages=ctx_managed_messages,
            system=system_message,
            model=model,
            stream=True)
        async for event in stream:
            event_dict = event.__dict__
            if "delta" in event_dict:
                yield getattr(event_dict.get("delta"), "text", "")
                # print(getattr(event_dict.get("delta"), "text", ""))

    @backoff.on_exception(backoff.expo, (RateLimitError, APIConnectionError),
                          max_time=MODEL_RETRIES)
    async def __complete__(self, model: str, messages: List[Dict], **kwargs):
        """
        Asynchronously completes the given message sequence by generating a response from the Claude model.

        Args:
            model (str): The model identifier to use.
            messages (List[Dict]): List of message dictionaries to send to the model.
            kwargs: Arbitrary keyword arguments that may include:
                - allowed_context_length (int): Maximum allowed length for the context. Defaults to 150,000 tokens.

        Returns:
            str: The generated response text.
        """
        system_message = list(
            filter(lambda message: message.get("role") == "system", messages))
        system_message = system_message[0].get("content")
        print(
            f"SYSTEM MESSAGE TOKENS: {self.ctx_mgmt.__count_tokens__(system_message)}"
        )
        messages = messages[1:]
        allowed_max_length = kwargs.get("allowed_context_length", 150_000)
        if "allowed_context_length" in kwargs:
            del kwargs["allowed_context_length"]
        else:
            logging.info(
                f"MAXIMUM ALLOWED CONTEXT LENGTH NOT SPECIFIED FOR OPENAI. Using 150,000"
            )
        ctx_managed_messages = self.ctx_mgmt(
            messages, allowed_max_length,
            self.ctx_mgmt.__count_tokens__(system_message))
        response = await self.tool.complete.client.messages.create(
            model=model,
            messages=ctx_managed_messages,
            system=system_message,
            **kwargs)
        # logging.info(f"RESPONSE: {response}")
        return response.content[0].text

    @backoff.on_exception(backoff.expo, (RateLimitError, APIConnectionError),
                          max_time=MODEL_RETRIES)
    async def __function_call__(self, model: str, messages: List[Dict],
                                tools: List[Dict], tool_choice: None | Dict,
                                **kwargs):
        """
        Asynchronously performs a function call by generating a response from the Claude model.

        Args:
            model (str): The model identifier to use.
            messages (List[Dict]): List of message dictionaries to send to the model.
            tools (List[Dict]): List of tools available for function calls.
            tool_choice (None | Dict): Specific tool choice if required.
            kwargs: Arbitrary keyword arguments that may include:
                - allowed_context_length (int): Maximum allowed length for the context. Defaults to 150,000 tokens.

        Returns:
            tuple: The name and parameters of the function to call if the output contains a function call.
            Otherwise, returns the output directly.
        """
        system_message = list(
            filter(lambda message: message.get("role") == "system", messages))
        system_message = system_message[0].get("content")
        # print(
        #     f"SYSTEM MESSAGE TOKENS: {self.ctx_mgmt.__count_tokens__(system_message)}"
        # )
        messages = messages[1:]
        allowed_max_length = kwargs.get("allowed_context_length", 150_000)
        if "allowed_context_length" in kwargs:
            del kwargs["allowed_context_length"]
        else:
            logging.info(
                f"MAXIMUM ALLOWED CONTEXT LENGTH NOT SPECIFIED FOR OPENAI. Using 150,000"
            )
        ctx_managed_messages = self.ctx_mgmt(
            messages, allowed_max_length,
            self.ctx_mgmt.__count_tokens__(system_message))
        output = await self.tool(model,
                                 ctx_managed_messages,
                                 tools,
                                 tool_choice,
                                 attach_system=system_message,
                                 **kwargs)
        return output
        # output = output[0]
        # if "name" in output:
        #     return output.get("name"), output.get("parameters")
        # else:
        #     return output
