import logging
from transformers import AutoTokenizer
from typing import List, Dict
from ._model_tokenizer_map import FileMap


class LLamaContextManagement:
    """
    A class to manage LLaMA context, tokenize input messages,
    and handle token count and padding.

    This class ensures that messages conform to a specific token length, manages
    system messages, and handles the creation of the final tokenized strings
    using a tokenizer model specified in FileMap.

    Attributes:
        tokenizer_model_name (str): Name of the tokenizer model to be used.
        tokenizer (AutoTokenizer): The tokenizer instance for the specified model.
    """

    def __init__(self, **kwargs):
        """
        Initializes an instance of LLamaContextManagement with the given model name.

        Args:
            kwargs: Arbitrary keyword arguments that may include:
                - model_name (str): The name of the model to be used. Must be within the available models in FileMap. Model name can be `llama3` or `llama3_1`.

        Raises:
            AssertionError: If `model_name` is not provided or is not an available model name.
        """
        assert "model_name" in kwargs and kwargs.get("model_name") in list(
            FileMap.models.keys()
        ), f"`model_name` parameter empty or provided model name is not available. Only the following models are available {list(FileMap.models.keys())}"
        self.tokenizer_model_name = kwargs.get("model_name")
        self.tokenizer = AutoTokenizer.from_pretrained(
            FileMap.models[self.tokenizer_model_name])

    def __count_tokens__(self, content: str):
        """
        Counts the number of tokens in the provided content string.

        Args:
            content (str): The text content to be tokenized and counted.

        Returns:
            int: The number of tokens in the content string.
        """
        return len(self.tokenizer.encode(content))

    def __pad_tokens__(self, content: str, max_length: int):
        """
        Pads or truncates the tokenized content to the specified maximum length.

        Args:
            content (str): The text content to be tokenized and padded/truncated.
            max_length (int): The maximum number of tokens allowed.

        Returns:
            str: The padded or truncated content string.
        """
        tokens = self.tokenizer.encode(content)[:max_length]
        return self.tokenizer.decode(tokens)

    def __call__(self, messages: List[Dict], max_length: int):
        """
        Processes a list of messages and returns a tokenized string
        conforming to the specified maximum token length.

        This method filters out system messages, reverses the message order,
        and ensures that the resulting tokenized string does not exceed the
        specified maximum length.

        Args:
            messages (List[Dict]): A list of messages dicts containing 'role' and 'content'.
            max_length (int): The maximum number of tokens allowed for the output string.

        Returns:
            str: A tokenized string created from the input messages.
        """
        system_message = list(
            filter(lambda message: message.get("role") == "system", messages))
        if len(system_message) > 0:
            system_message = system_message[0]
        else:
            system_message = None
        filtered_messages = list(
            filter(lambda message: message.get("role") != "system", messages))
        filtered_messages = filtered_messages[::-1]
        managed_messages = []
        previous_role = None
        current_token_num = 0
        for ix, message in enumerate(filtered_messages):
            content = message.get("content")
            role = message.get("role")
            num_tokens = self.__count_tokens__(content)
            if current_token_num + num_tokens >= max_length:
                num_tokens_to_keep = max_length - current_token_num - 1
                if num_tokens_to_keep <= 0:
                    break
                content = self.__pad_tokens__(content, num_tokens_to_keep)
            if ix > 0:
                if previous_role == role:
                    managed_messages[-1] += f"\n{content}"
                else:
                    managed_messages.append({"content": content, "role": role})
            else:
                managed_messages.append({"content": content, "role": role})
            previous_role = role
            current_token_num += self.__count_tokens__(content)
        managed_messages = managed_messages[::-1]
        if system_message:
            system_message_tokens = self.__count_tokens__(
                system_message.get("content"))
            managed_messages = [system_message] + managed_messages
        else:
            system_message_tokens = 0
        logging.debug(f"SYSTEM MESSAGE TOKENS: {system_message_tokens}")
        logging.debug(
            f"TOTAL TOKENS: {current_token_num + system_message_tokens}")
        print(f"SYSTEM MESSAGE TOKENS: {system_message_tokens}")
        print(
            f"TOTAL TOKENS: {current_token_num + system_message_tokens}")
        return self.tokenizer.apply_chat_template(managed_messages,
                                                  tokenize=False)
