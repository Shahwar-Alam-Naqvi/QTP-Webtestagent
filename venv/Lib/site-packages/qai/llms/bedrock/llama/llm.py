import json
import logging
from qai.llms.base import BaseLLM
from typing import List, Dict
from .._session import AIOBotoSession
from qai.configs import (AWS_BEDROCK_ACCESS_KEY, AWS_BEDROCK_SECRET_KEY,
                         AWS_BEDROCK_REGION)
from ..context.manager import LLamaContextManagement
from ..context._model_tokenizer_map import FileMap
from ..tools.tool import Tool


class LlamaLLM(BaseLLM):
    """
    Llama LLM

    A class that handles interaction with the Llama model family using AWS services.
    This class requires AWS credentials and a specified model family to function correctly.
    It can either use provided credentials or those defined in the environment variables.

    Environment Variables:
        - AWS_BEDROCK_ACCESS_KEY: The AWS access key ID.
        - AWS_BEDROCK_SECRET_KEY: The AWS secret access key.
        - AWS_BEDROCK_REGION: The AWS region name.
    """

    def __init__(self, **kwargs):
        """
        Initializes an instance of LlamaLLM with the provided arguments or environment variables.

        Args:
            kwargs: Arbitrary keyword arguments that may include:
                - aws_access_key_id (str): AWS access key ID.
                - aws_secret_access_key (str): AWS secret access key.
                - region_name (str): AWS region name.
                - model_family (str): The model family to use. Must be provided.

        Raises:
            AssertionError: If `model_family` is not provided or invalid.

        Note:
            If aws_access_key_id, aws_secret_access_key, or region_name are not provided,
            the respective environment variables will be used:
                - AWS_BEDROCK_ACCESS_KEY
                - AWS_BEDROCK_SECRET_KEY
                - AWS_BEDROCK_REGION
        """
        assert "model_family" in kwargs, f"`model_family` is not provided or the provided model family is not supported. The following are the supported model families {list(FileMap.models.keys())}"
        self.aboto_session_obj = AIOBotoSession(
            aws_access_key_id=kwargs.get("aws_access_key_id",
                                         AWS_BEDROCK_ACCESS_KEY),
            aws_secret_access_key=kwargs.get("aws_secret_access_key",
                                             AWS_BEDROCK_SECRET_KEY),
            region_name=kwargs.get("region_name", AWS_BEDROCK_REGION))
        self.__initialize_context_manager__(kwargs.get("model_family"))
        self.tool_patch = Tool()

    def __initialize_context_manager__(self, model_name: str):
        """
        Initializes the context manager for the specified model family.

        Args:
            model_name (str): The name of the model family to initialize the context manager with.
        """
        self.llama_model_family = model_name
        self.context = LLamaContextManagement(
            model_name=self.llama_model_family)

    def __create_request_body__(self, messages: List[Dict], **kwargs):
        """
        Creates the request body for sending messages to the Llama model.

        Args:
            messages (List[Dict]): The list of message dictionaries.
            kwargs: Arbitrary keyword arguments including:
                - model_family (str): The model family to use.
                - allowed_context_length (int): The maximum allowed context length. Defaults to 6000 if not provided.

        Returns:
            str: The JSON request body.

        Raises:
            ValueError: If `model_family` is invalid.
        """
        if "model_family" in kwargs and kwargs.get(
                "model_family") != self.llama_model_family:
            if kwargs.get("model_family") not in list(FileMap.models.keys()):
                raise ValueError(
                    f"`model_family` argument is invalid. Only the following model families are supported: {list(FileMap.models.keys())}"
                )
            else:
                self.__initialize_context_manager__(kwargs.get("model_family"))

        allowed_max_length = kwargs.get("allowed_context_length", 6_000)
        if "allowed_context_length" in kwargs:
            del kwargs["allowed_context_length"]
        else:
            logging.info(
                f"MAXIMUM ALLOWED CONTEXT LENGTH NOT SPECIFIED FOR LLAMA. Using 6,000"
            )
        input_str = self.context(messages, allowed_max_length)
        request_body = json.dumps({"prompt": input_str, **kwargs})
        return request_body

    async def __stream__(self, model: str, messages: List[Dict], **kwargs):
        """
        Streams responses from the Llama model.

        Args:
            model (str): The model ID to use.
            messages (List[Dict]): The list of message dictionaries to send.
            kwargs: Arbitrary keyword arguments.

        Yields:
            dict: The streamed response from the model.

        Raises:
            ValueError: If the response format is unexpected.
        """
        request_body = self.__create_request_body__(messages, **kwargs)
        async with self.aboto_session_obj.aboto_session.client(
                "bedrock-runtime",
                config=self.aboto_session_obj.retry_config) as client:
            response = await client.invoke_model_with_response_stream(
                body=request_body,
                modelId=model,
                contentType="application/json",
                accept="application/json")
            if 'body' in response:
                async for event in response["body"]:
                    delta = json.loads(
                        event.get("chunk").get("bytes").decode()).get(
                            "generation")
                    # delta = json.loads(
                    #     event.get("chunk").get("bytes").decode()).get(
                    #         "outputs")[0].get("text")
                    yield delta
            else:
                yield "Something went wrong"
                raise ValueError("Unexpected response format")

    async def __complete__(self, model: str, messages: List[Dict], **kwargs):
        """
        Completes the response from the Llama model.

        Args:
            model (str): The model ID to use.
            messages (List[Dict]): The list of message dictionaries to send.
            kwargs: Arbitrary keyword arguments.

        Returns:
            dict: The response from the model.

        Raises:
            ValueError: If the response format is unexpected.
        """
        request_body = self.__create_request_body__(messages, **kwargs)
        async with self.aboto_session_obj.aboto_session.client(
                "bedrock-runtime",
                config=self.aboto_session_obj.retry_config) as client:
            response = await client.invoke_model(
                body=request_body,
                modelId=model,
                accept="application/json",
                contentType="application/json")
            if 'body' in response:
                response_body_str = await response['body'].read()
                response_body = json.loads(response_body_str)
                # return response_body.get("outputs")[0].get("text")
                return response_body.get("generation")
            else:
                raise ValueError("Unexpected response format")

    async def __function_call__(self, model: str, messages: List[Dict],
                                tools: List[Dict], tool_choice: None | Dict,
                                **kwargs):
        """
        Executes a function call using the Llama model and tools.

        Args:
            model (str): The model ID to use.
            messages (List[Dict]): The list of message dictionaries to send.
            tools (List[Dict]): The list of tools to use.
            tool_choice (None | Dict): The chosen tool, if any.
            kwargs: Arbitrary keyword arguments.

        Returns:
            dict: The result of the function call.
        """
        system_message = list(
            filter(lambda message: message.get("role") == "system", messages))
        if len(system_message) > 0:
            system_message = system_message[0].get("content")
        else:
            system_message = None
        non_system_messages = list(
            filter(lambda message: message.get("role") != "system", messages))
        function_output = await self.tool_patch.tool_call(
            model,
            non_system_messages,
            self.__complete__,
            tools,
            tool_choice,
            attach_system=system_message,
            **kwargs)
        return function_output
