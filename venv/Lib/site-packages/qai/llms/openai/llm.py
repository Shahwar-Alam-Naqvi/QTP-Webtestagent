import logging
import json
import traceback
import backoff
import json_repair
from qai.llms.base import BaseLLM
from qai.llms.openai.tokenizer.context import OpenAIContextManagement
from openai import (AsyncAzureOpenAI, AsyncOpenAI, RateLimitError,
                    APIConnectionError)
from typing import List, Dict, Union, Tuple
from qai.configs import (OPENAI_API_TYPE, AZURE_OPENAI_ENDPOINTS_KEYS,
                         AZURE_DEPLOYMENT_NAMES, OPENAI_API_VERSION,
                         MODEL_RETRIES)


class OpenAILLM(BaseLLM):
    """
    OpenAI LLM Class for handling OpenAI model interactions.

    This class provides methods to interface with OpenAI models for various types of requests such
    as streaming, completion, and function calls. It uses an OpenAI client to communicate with the API.

    Environmental variables:
        - OPENAI_API_TYPE: The type of OpenAI API ("azure" by default).
        - AZURE_OPENAI_ENDPOINTS: The Azure OpenAI endpoints.
        - AZURE_OPENAI_KEYS: The Azure OpenAI keys.
        - OPENAI_API_VERSION: The version of the OpenAI API to use.

    Attributes:
        client (AsyncOpenAI/AsyncAzureOpenAI): The OpenAI client instance.
        extra_args (dict): Dictionary of extra arguments.
        endpoint_key_pairs (list): List of endpoint and key pairs for Azure OpenAI.
    """

    def __init__(self, **kwargs):
        """
        Initializes an instance of OpenAILLM with the provided arguments or environment variables.

        Args:
            kwargs: Arbitrary keyword arguments that may include:
                - openai_endpoint_key_pairs (list): List of endpoint and key pairs for Azure OpenAI.

        Attributes:
            client (AsyncOpenAI/AsyncAzureOpenAI): The OpenAI client instance, which will be initialized later.
            extra_args (dict): Dictionary of extra arguments.
            endpoint_key_pairs (list): List of endpoint and key pairs for Azure OpenAI.
        """
        self.client = None
        self.extra_args = kwargs
        self.endpoint_key_pairs = kwargs.get("openai_endpoint_key_pairs",
                                             AZURE_OPENAI_ENDPOINTS_KEYS)

    async def create_openai_client(self, endpoint_key_pair):
        """
        Creates an OpenAI client instance based on the provided endpoint and API key pairs.

        Args:
            endpoint_key_pair (Tuple[str, str]): A pair consisting of the API endpoint and API key.

        Initializes:
            - An instance of either AsyncAzureOpenAI or AsyncOpenAI based on OPENAI_API_TYPE environment variable.
        """
        api_endpoint, api_key = endpoint_key_pair
        if OPENAI_API_TYPE == "azure":
            self.client = AsyncAzureOpenAI(azure_endpoint=api_endpoint,
                                           api_version=self.extra_args.get(
                                               "openai_api_version",
                                               OPENAI_API_VERSION),
                                           api_key=api_key)
        else:
            self.client = AsyncOpenAI(api_key=api_key)

    async def __stream__(self, model: str, messages: List[Dict], **kwargs):
        """
        Streams responses from the OpenAI model.

        Args:
            model (str): The model to use for generating responses.
            messages (List[Dict]): A list of messages to be sent to the model.
            kwargs: Additional arguments for the OpenAI client, including 'allowed_context_length'.

        Yields:
            str: The content of the response messages streamed from the model.

        Remarks:
            If 'allowed_context_length' is not specified, defaults to 120,000 tokens.
        """
        self.ctx_mgmt = OpenAIContextManagement(model_name=model)
        for endpoint_key_pair in self.endpoint_key_pairs:
            await self.create_openai_client(endpoint_key_pair)
            try:
                allowed_max_length = kwargs.get("allowed_context_length",
                                                120_000)
                if "allowed_context_length" in kwargs:
                    del kwargs["allowed_context_length"]
                else:
                    logging.info(
                        f"MAXIMUM ALLOWED CONTEXT LENGTH NOT SPECIFIED FOR OPENAI. Using 120,000"
                    )
                ctx_managed_messages = self.ctx_mgmt(messages,
                                                     allowed_max_length)
                stream = await self.client.chat.completions.create(
                    model=AZURE_DEPLOYMENT_NAMES[model],
                    messages=ctx_managed_messages,
                    temperature=0.2,
                    stream=True,
                    **kwargs)
                async for chunk in stream:
                    if len(chunk.choices) > 0:
                        yield chunk.choices[0].delta.content or ""
                break
            except RateLimitError as err:
                logging.info(f"RateLimitError: {str(err)}")
                continue
            except Exception as err:
                logging.exception(f"OPENAI STREAM ERROR: {str(err)}")
                yield "Something went wrong"
                break

    @backoff.on_exception(backoff.expo,
                          RateLimitError,
                          max_tries=MODEL_RETRIES)
    async def __attempt_with_client__(self, model, messages, **kwargs):
        """
        Attempts to get a response from the OpenAI client with a backoff strategy.

        Args:
            model (str): The model to use for generating responses.
            messages (List[Dict]): The messages to be sent to the model.
            kwargs: Additional arguments for the OpenAI client, including 'allowed_context_length'.

        Returns:
            str: The content of the response message.

        Remarks:
            If 'allowed_context_length' is not specified, defaults to 120,000 tokens.
        """
        allowed_max_length = kwargs.get("allowed_context_length", 120_000)
        if "allowed_context_length" in kwargs:
            del kwargs["allowed_context_length"]
        else:
            logging.info(
                f"MAXIMUM ALLOWED CONTEXT LENGTH NOT SPECIFIED FOR OPENAI. Using 120,000"
            )
        ctx_managed_messages = self.ctx_mgmt(messages, allowed_max_length)
        # print(MODELS[model])
        is_function_call = kwargs.get("is_function_call", False)
        if "is_function_call" in kwargs:
            del kwargs["is_function_call"]
        response = await self.client.chat.completions.create(
            model=AZURE_DEPLOYMENT_NAMES[model],
            messages=ctx_managed_messages,
            temperature=0.2,
            **kwargs)
        if is_function_call:
            return response.choices[0].message
        return response.choices[0].message.content

    async def __complete__(self, model: str, messages: List[Dict], **kwargs):
        """
        Completes a set of messages using the OpenAI model.

        Args:
            model (str): The model to use for generating responses.
            messages (List[Dict]): The messages to be completed by the model.
            kwargs: Additional arguments for the OpenAI client, including 'allowed_context_length'.

        Returns:
            str: The content of the response message.

        Remarks:
            If 'allowed_context_length' is not specified, defaults to 120,000 tokens.
        """
        self.ctx_mgmt = OpenAIContextManagement(model_name=model)
        for endpoint_key_pair in self.endpoint_key_pairs:
            # print(f"ENDPOINT: {endpoint_key_pair}")
            await self.create_openai_client(endpoint_key_pair)
            result = await self.__attempt_with_client__(
                model, messages, **kwargs)
            logging.info(f"RESULT: {result}")
            return result

    async def __function_call__(self, model: str, messages: List[Dict],
                                tools: List[Dict],
                                tool_choice: Union[None, Dict], **kwargs):
        """
        Executes a function call using the OpenAI model.

        Args:
            model (str): The model to use for generating responses.
            messages (List[Dict]): The messages to be analyzed by the model.
            tools (List[Dict]): A list of tools (functions) to be used by the model.
            tool_choice (Union[None, Dict]): The specific tool to be used, if any.
            kwargs: Additional arguments for the OpenAI client, including 'allowed_context_length'.

        Returns:
            Union[str, Tuple[str, Dict]]: The name and parameters of the function to be called.

        Remarks:
            If 'allowed_context_length' is not specified, defaults to 120,000 tokens.
        """
        functions = tools
        function_call = tool_choice
        tools = [{
            "type": "function",
            "function": function
        } for function in functions]
        if function_call:
            tool_choice = {"type": "function", "function": function_call}
        output = await self.__complete__(model,
                                         messages,
                                         tools=tools,
                                         tool_choice=tool_choice,
                                         is_function_call=True,
                                         **kwargs)
        logging.info(f"FC OUTPUT OUTPUT: {output}")
        output = output.tool_calls[0]
        function_name = output.function.name
        try:
            arguments = json.loads(output.function.arguments)
            return [{"name": function_name, "parameters": arguments}]
            # return function_name, arguments
        except json.decoder.JSONDecodeError as error:

            try:
                arguments = json_repair.loads(output.function.arguments)
                return function_name, arguments
            except json.decoder.JSONDecodeError as error:
                logging.info(f'JSON DECODER ERROR: \n{traceback.format_exc()}')
                messages += [{
                    "role":
                    "assistant",
                    "content":
                    f"Assistant Output: {output.function.arguments}"
                }, {
                    "role":
                    "user",
                    "content":
                    f"Please provide a valid JSON string. Getting Error: {str(error)}"
                }]
                return await self.__function_call__(model, messages, tools,
                                                    tool_choice)
        except Exception as err:
            logging.exception(f'Exception \n{traceback.format_exc()}')
            raise Exception
